{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Chest X-rays with Cross-Modal Data Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to use the *cross-modal data programming* technique described in Dunnmon and Ratner, et al. (2019) to build a Convolutional Neural Network (CNN) model for chest radiograph triage with no hand-labeled training data that performs similarly to a CNN supervised using several thousand data points labeled by radiologists. \n",
    "\n",
    "In cross-modal data programming, we model and combine expert-provided heuristics written over an auxiliary modality (e.g. a text report), which is only available at training time, to create probabilistic labels for training a machine learning model over a target modality (e.g. a chest radiograph).  \n",
    "\n",
    "Below, we provide a step-by-step walkthrough of how to apply this technique to a small, publicly available chest radiograph dataset.  This process is equivalent to that followed for each dataset in our 2019 submission. \n",
    "\n",
    "We use high-level APIs from the [Snorkel MeTaL](https://github.com/HazyResearch/metal) software package (imported as `metal`) to support heuristic development and model training.  Additional documentation and information about the underlying techniques can be found in papers from [NeurIPS 2016](https://arxiv.org/pdf/1605.07723.pdf), [VLDB 2017](http://www.vldb.org/pvldb/vol11/p269-ratner.pdf), and [AAAI 2019](https://arxiv.org/pdf/1810.02840.pdf), or at [snorkel.stanford.edu](snorkel.stanford.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setting Up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by setting up our notebook environment and importing relevant Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "# Making sure CUDA devices are visible\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "# Importing pandas for data processing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading and Splitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up the data dictionary and load data that we've already split for you into an (approximately) 80% train split, 10% development split, and 10% test split.  Each raw data point contains three fields: a text report, a label (normal or abnormal), and a set of image paths.  The original data, from the OpenI dataset, is maintained by [NIH](https://openi.nlm.nih.gov/faq.php)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up data dictionary and defining data splits\n",
    "data = {}\n",
    "splits = ['train','dev','test']\n",
    "\n",
    "for split in splits:\n",
    "    data[split] = pd.read_csv(f'data/{split}_entries.csv')[['label','xray_paths','text']]\n",
    "    # Adjusting labels to fit with Snorkel MeTaL labeling convention (0 reserved for abstain)\n",
    "    data[split]['label'][data[split]['label']==0] = 2\n",
    "    perc_pos = sum(data[split]['label']==1)/len(data[split])\n",
    "    print(f'{len(data[split])} {split} examples: {100*perc_pos:0.1f}% Abnormal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see an example of a single data point below -- note that the raw label convention for our normal vs. abnormal classification problem is 1 for abnormal and 2 for normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a single sample from the dataframe\n",
    "# Change sample_index to see different examples\n",
    "\n",
    "sample_index = 0\n",
    "sample = data['train'].iloc[sample_index]\n",
    "print('RAW TEXT:\\n \\n',sample['text'],'\\n')\n",
    "print('IMAGE PATHS: \\n \\n', sample['xray_paths'],'\\n')\n",
    "print('LABEL:', sample['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Developing LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our *labeling functions* (LFs): simple, heuristic functions written by a domain expert (e.g., a radiologist) that correctly label a report as normal or abnormal with probability better than random chance.  \n",
    "\n",
    "We give an example of all three types of LFs we reference in our paper: general pattern LFs that operate on patterns a non-expert user could easily identify, medical pattern LFs that operate on patterns easily identifiable by a clinician, and structural LFs that focus on specific structural elements of the report (e.g. how long it is) that have some correlation with the scan it describes being normal or abnormal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Value to use for abstain votes\n",
    "ABSTAIN = 0\n",
    "# Value to use for abnormal votes\n",
    "ABNORMAL = 1\n",
    "# Value to user for normal votes\n",
    "NORMAL = 2\n",
    "\n",
    "# Example of a General Pattern LF\n",
    "def LF_is_seen_or_noted_in_report_demo(report):\n",
    "    if any(word in report.lower() for word in [\"is seen\", \"noted\"]):\n",
    "        return ABNORMAL\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "# Example of a Medical Pattern LF\n",
    "def LF_lung_hyperdistention_demo(report):\n",
    "    \"\"\"\n",
    "    Votes abnormal for indications of lung hyperdistention.\n",
    "    \"\"\"\n",
    "    reg_01 = re.compile(\"increased volume|hyperexpan|inflated\", re.IGNORECASE)\n",
    "    for s in report.split(\".\"):\n",
    "        if reg_01.search(s):\n",
    "            return ABNORMAL\n",
    "    ### *** ###\n",
    "    return NORMAL\n",
    "\n",
    "# Example of a Structural LF\n",
    "def LF_report_is_short_demo(report):\n",
    "    \"\"\"\n",
    "    Checks if report is short.\n",
    "    \"\"\"\n",
    "    return NORMAL if len(report) < 280 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see how well these LFs might do at correctly indicating normal or abnormal examples.  Check them out by changing the `lf_test` function in the cell below to reference one of those listed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from metal.analysis import single_lf_summary, confusion_matrix\n",
    "\n",
    "# Testing single LF\n",
    "lf_test = LF_lung_hyperdistention_demo\n",
    "\n",
    "# Computing labels\n",
    "Y_lf = np.array([lf_test(doc['text']) for ind, doc in data['dev'].iterrows()])\n",
    "Y_dev = np.array([doc['label'] for ind, doc in data['dev'].iterrows()])\n",
    "\n",
    "# Summarizing LF performance\n",
    "single_lf_summary(Y_lf, Y=Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use analyze the `LF_lung_hyperdistention_demo` function -- in this case,  we see that it has polarity [1,2], meaning it votes on both class 1 and class 2 (and votes on every example because `coverage` = 1.0), but that it has low accuracy (around 44%).  Let's look at the confusion matrix to see why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix\n",
    "conf = confusion_matrix(Y_dev, Y_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this LF is much more accurate on abnormal examples (where y=1) than on normal examples (where y=2).  Why don't we adjust it to only vote in the positive direction and see how we do?  \n",
    "\n",
    "Go ahead and change `NORMAL` to `ABSTAIN` in the `LF_lung_hyperdistention_demo` function (the line below the `### *** ###` comment), and rerun the last three code cells.  \n",
    "\n",
    "You'll see that by making this rule a bit more targeted, its coverage decreases to 9%, but it's accuracy jumps to over 90%.  This type of iteration is exactly how clinicians can develop LFs in practice.\n",
    "\n",
    "You may also notice that it's very easy to write these LFs over text, but it would be very hard to, say, write an `LF_lung_hyperdistention` version that operates over an image -- this is why cross-modality is so important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Computing the Label Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've designed a couple of LFs, it's time to execute them all on every example we have to create a *label matrix*.  This is an $n$ by $m$ matrix, where $n$ is the number of examples and $m$ is the number of LFs.  \n",
    "\n",
    "Below, we've provided more LFs to give you a sense of a what a real application would look like -- code for these can be found in the `labeling_functions.py` file in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labeling_functions import *\n",
    "\n",
    "lfs = [\n",
    "    LF_report_is_short,\n",
    "    LF_consistency_in_report,\n",
    "    LF_negative_inflection_words_in_report,\n",
    "    LF_is_seen_or_noted_in_report,\n",
    "    LF_disease_in_report,\n",
    "    LF_abnormal_mesh_terms_in_report,\n",
    "    LF_recommend_in_report,\n",
    "    LF_mm_in_report,\n",
    "    LF_normal,\n",
    "    LF_positive_MeshTerm,\n",
    "    LF_fracture,\n",
    "    LF_calcinosis,\n",
    "    LF_degen_spine,\n",
    "    LF_lung_hypoinflation,\n",
    "    LF_lung_hyperdistention,\n",
    "    LF_catheters,\n",
    "    LF_surgical,\n",
    "    LF_granuloma,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a few simple helper functions for running our labeling functions over all text reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def evaluate_lf_on_docs(docs, lf):\n",
    "    \"\"\"\n",
    "    Evaluates lf on list of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    lf_list = []\n",
    "    for doc in docs:\n",
    "        lf_list.append(lf(doc))\n",
    "    return lf_list\n",
    "\n",
    "def create_label_matrix(lfs, docs):\n",
    "    \"\"\"\n",
    "    Creates label matrix from documents and lfs\n",
    "    \"\"\"\n",
    "    \n",
    "    delayed_lf_rows = []\n",
    "    \n",
    "    for lf in lfs:\n",
    "        delayed_lf_rows.append(dask.delayed(evaluate_lf_on_docs)(docs, lf))\n",
    "\n",
    "    with ProgressBar():\n",
    "        L = csr_matrix(np.vstack(dask.compute(*delayed_lf_rows)).transpose())  \n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we simply apply each of our LFs to each of our reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lf names\n",
    "lf_names = [lf.__name__ for lf in lfs]\n",
    "\n",
    "# Allocating label matrix and ground truth label lists\n",
    "Ls = []\n",
    "Ys = []\n",
    "\n",
    "# Computing lfs\n",
    "print('Computing label matrices...')\n",
    "for i, docs in enumerate((\n",
    "    data['train']['text'].tolist(), \n",
    "    data['dev']['text'].tolist(), \n",
    "    data['test']['text'].tolist()\n",
    "    )\n",
    "):\n",
    "    Ls.append(create_label_matrix(lfs,docs))  \n",
    "\n",
    "# Getting ground truth labels\n",
    "print('Creating ground truth label vectors...')\n",
    "Ys = [data['train']['label'].tolist(),\n",
    "      data['dev']['label'].tolist(),\n",
    "      data['test']['label'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done this, we can inspect our accuracy on the development set and other useful LF metrics using the simple Snorkel MeTaL interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.analysis import lf_summary\n",
    "\n",
    "# Analyzing LF stats\n",
    "lf_summary(Ls[1], Y=Y_dev, lf_names=lf_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of our labeling functions, while certainly imperfect, are better than random chance.  This fulfills the only theoretical requirement of the cross-modal data programming algorithm.  \n",
    "\n",
    "We can also get a sense of where the LFs overlap and conflict by inspecting the following plot; it is useful that some of the LFs overlap or conflict, as this provides signal that allows us to learn their accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  metal.contrib.visualization.analysis import view_conflicts\n",
    "\n",
    "# Viewing conflicts\n",
    "view_conflicts(Ls[1], normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train a Label Model in Snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the Snorkel MeTaL model training API (imported as `metal`) to train a generative model that learns the accuracies of our LFs.  By combining our labeling functions based on their accuracies, we can recover a model that outputs higher quality\n",
    "weak labels.\n",
    "\n",
    "We perform a simple random hyperparameter search over learning rate and L2 regularization, using our small labeled development set to choose the best model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.label_model import LabelModel\n",
    "from metal.logging import LogWriter\n",
    "from metal.tuners import RandomSearchTuner\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Creating search space\n",
    "search_space = {\n",
    "        'l2': {'range': [0.0001, 0.1], 'scale':'log'},           # linear range\n",
    "        'lr': {'range': [0.0001, 0.1], 'scale': 'log'},  # log range\n",
    "        }\n",
    "\n",
    "searcher = RandomSearchTuner(LabelModel, log_dir='./run_logs',\n",
    "            log_writer_class=None)\n",
    "\n",
    "# Training generative model\n",
    "gm = searcher.search(search_space, (Ls[1],Ys[1]), \\\n",
    "        train_args=[Ls[0]], init_args=[],\n",
    "        init_kwargs={'k':2, 'seed':1701}, \n",
    "        train_kwargs={'n_epochs':200},\n",
    "        max_search=20,\n",
    "        verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our best model on the development set as below -- you should recover a model with best accuracy of approximately 85% on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GM SCORING CELL\n",
    "# Getting scores\n",
    "scores = gm.score((Ls[1], Ys[1]), metric=['accuracy','precision', 'recall', 'f1','roc-auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this useful?  If we compare to majority vote, we see a couple points of improvement in accuracy.  Note that the degree to which we expect this model to improve over majority vote varies based on the type of dataset involved, as detailed in the 2017 [VLDB Paper](http://www.vldb.org/pvldb/vol11/p269-ratner.pdf) describing the Snorkel system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.label_model.baselines import MajorityLabelVoter\n",
    "\n",
    "# Checking if we beat majority vote\n",
    "mv = MajorityLabelVoter(seed=123)\n",
    "scores = mv.score((Ls[1], Ys[1]), metric=['accuracy', 'precision', 'recall', 'f1', 'roc-auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Apply Heuristic Optimizer for LSTM Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the cross-modal data programming heuristic optimizer to determine whether or not to train an LSTM mapping the raw report text to the generative model output.  We would train such a model using standard tools from PyTorch in cases where either coverage or ROC-AUC of the generative model on the development set is less than 90%.  \n",
    "\n",
    "For this example, our coverage is 100%, and generative model (`gm`) ROC-AUC should be around 92% on the development set (see cell above with comment `GM SCORING CELL`).  Thus, we forego training the LSTM, and use our generative model directly to provide weak labels for our target modality model.  This saves substantial computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create a Weakly Labeled Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this trained generative model to create weak labels for each of our train, development, and test splits by applying it to the label matrices, as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_ps = gm.predict_proba(Ls[0])\n",
    "Y_dev_ps = gm.predict_proba(Ls[1])\n",
    "Y_test_ps = gm.predict_proba(Ls[2])\n",
    "Y_ps = [Y_train_ps, Y_dev_ps, Y_test_ps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the distribution of our weak training labels, and note that they are assigned varying degrees of probability.  An advantage of this labeling approach is that probabilistic labels can be very descriptive -- i.e., if an example has a 60% probability of being abnormal, we train against that 0.6 probability, rather than binarizing to 100%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  metal.contrib.visualization.analysis  import plot_probabilities_histogram\n",
    "\n",
    "# Looking at probability histogram for training labels\n",
    "plot_probabilities_histogram(Y_dev_ps[:,0], title=\"Probablistic Label Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the development set, we can also check that the class balance of our weak labels if we were to naively binarize at the 0.5 cutoff -- we see reasonable behavior here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.contrib.visualization.analysis import plot_predictions_histogram\n",
    "# Obtaining binarized predictions\n",
    "Y_dev_p = gm.predict(Ls[1])\n",
    "plot_predictions_histogram(Y_dev_p, Ys[1], title=\"Label Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train a Weakly Supervised Target Modality Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our weak training labels, we can train a commodity CNN using the high-level PyTorch API From Snorkel MeTaL.  The entire process of defining and training the model can be executed in the following two simple cells.\n",
    "\n",
    "First, we define PyTorch `DataLoader` objects to efficiently load our image data, associating each image with the weak label generated from its associated report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from metal.end_model import EndModel\n",
    "from metal.logging.tensorboard import TensorBoardWriter\n",
    "from utils import get_data_loader\n",
    "\n",
    "# Setting up log directory\n",
    "log_config = {\"log_dir\": \"./run_logs\", \"run_name\": 'openi_demo_ws'}\n",
    "tuner_config = {\"max_search\": 1}\n",
    "search_space = {\n",
    "    \"l2\": [0.0005],  # linear range\n",
    "    \"lr\": [0.001]\n",
    "}\n",
    "\n",
    "# Create pytorch model\n",
    "num_classes = 2\n",
    "cnn_model = models.resnet18(pretrained=True)\n",
    "last_layer_input_size = int(cnn_model.fc.weight.size()[1])\n",
    "cnn_model.fc = torch.nn.Linear(last_layer_input_size, num_classes)\n",
    "\n",
    "# Create data loaders\n",
    "loaders = {}\n",
    "loaders['train'] = get_data_loader(data['train']['xray_paths'].tolist(), Y_ps[0], batch_size=32, shuffle=True)\n",
    "loaders['dev'] = get_data_loader(data['dev']['xray_paths'].tolist(), Ys[1], batch_size=32, shuffle=False)\n",
    "loaders['test'] = get_data_loader(data['test']['xray_paths'].tolist(), Ys[2], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, a single datapoint yields an image like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img, label = loaders['train'].dataset[0]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img[0,:,:],cmap='gray')\n",
    "plt.title('Example X-ray Image')\n",
    "ax = plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our `DataLoaders` are set up, it is a simple matter to define and train our CNN model. \n",
    "\n",
    "Note: While this will run if you do not have a CUDA-based GPU available (and will automatically detect it if you do), it will proceed *much* faster if you have one!  CPU-only per-epoch training time is ~ 15 minutes, while with a Titan X it is approximately 30 s!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining network parameters\n",
    "num_classes = 2\n",
    "pretrained = True\n",
    "train_args = [loaders['train']]\n",
    "init_args = [[num_classes]]\n",
    "\n",
    "# Defining device variable\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initializing input module\n",
    "input_module = cnn_model\n",
    "init_kwargs = {\"input_module\": input_module, \n",
    "               \"skip_head\": True,\n",
    "               \"input_relu\": False,\n",
    "               \"input_batchnorm\": False,\n",
    "               \"device\": device,\n",
    "               'seed':1701}\n",
    "train_kwargs = {'n_epochs': 5,\n",
    "                'progress_bar':True}\n",
    "\n",
    "# Setting up logger and searcher\n",
    "searcher = RandomSearchTuner(EndModel, \n",
    "    **log_config, log_writer_class=TensorBoardWriter, \n",
    "    validation_metric='accuracy',\n",
    "    seed=1701)\n",
    "\n",
    "# Training weakly supervised model\n",
    "weakly_supervised_model = searcher.search(\n",
    "    search_space,\n",
    "    loaders['dev'],\n",
    "    train_args=train_args,\n",
    "    init_args=init_args,\n",
    "    init_kwargs=init_kwargs,\n",
    "    train_kwargs=train_kwargs,\n",
    "    max_search=tuner_config[\"max_search\"],\n",
    "    clean_up=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate this model below, and see that we've learned some useful signal!  Remember that an Area Under the Receiver Operating Characteristic (ROC-AUC) score represents the probability across all possible cutoffs of ranking an abnormal example higher than a normal example.  If we've learned nothing useful, this value would be 0.5. \n",
    "\n",
    "You should expect a value just around 0.70 for this training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model\n",
    "print (f\"Evaluating Weakly Supervised Model\")\n",
    "scores = weakly_supervised_model.score(\n",
    "    loaders['test'], metric=[\"roc-auc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Comparing to a Fully Supervised Target Modality Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have ground-truth labels for the entire dataset in this case (the OpenI dataset comes with these labels, which require physicians to label thousands of images!), we can compare how well our weakly supervised target modality modeldoes with the performance we achieve from a fully supervised model.  This is a similar analysis to that performed in our 2019 submission.\n",
    "\n",
    "Executing this requires a simple change to the training dataloader to provide it with ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating logging config\n",
    "log_config = {\"log_dir\": \"./run_logs\", \"run_name\": 'openi_demo_fs'}\n",
    "\n",
    "\n",
    "# Creating dataloader with ground truth training labels\n",
    "loaders['full_train'] = get_data_loader(data['train']['xray_paths'].tolist(), Ys[0], batch_size=32, shuffle=True)\n",
    "train_args = [loaders['full_train']]\n",
    "\n",
    "# Setting up logger and searcher\n",
    "searcher = RandomSearchTuner(EndModel, \n",
    "    **log_config, log_writer_class=TensorBoardWriter, \n",
    "    validation_metric='accuracy',\n",
    "    seed=1701)\n",
    "\n",
    "# Training                             \n",
    "fully_supervised_model = searcher.search(\n",
    "    search_space,\n",
    "    loaders['dev'],\n",
    "    train_args=train_args,\n",
    "    init_args=init_args,\n",
    "    init_kwargs=init_kwargs,\n",
    "    train_kwargs=train_kwargs,\n",
    "    max_search=tuner_config[\"max_search\"],\n",
    "    clean_up=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can evaluate the weakly and fully supervised models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating weakly model\n",
    "print (f\"Evaluating Weakly Supervised Model\")\n",
    "weakly_supervised_scores = weakly_supervised_model.score(\n",
    "    loaders['test'], metric=[\"roc-auc\"], print_confusion_matrix=False,\n",
    ")\n",
    "\n",
    "# Evaluating fully supervised model\n",
    "print (f\"Evaluating Fully Supervised Model\")\n",
    "fully_supervised_scores = fully_supervised_model.score(\n",
    "    loaders['test'], metric=[\"roc-auc\"], print_confusion_matrix=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the models have trained successfully, you should observe that the weakly and fully supervised models both achieve ROC-AUC scores around 0.70-0.75.  This indicates that the weak labels we created using our labeling functions over the text have successfully allowed us to train a CNN model that performs similarly to one trained using ground truth, but *without having to label thousands of images*.  \n",
    "\n",
    "Further, we would expect the performance of the weakly supervised model to improve as more *unlabeled* data is added to this relatively small dataset!\n",
    "\n",
    "Congratulations! You've just trained a deep learning model using cross-modal data programming!  You can learn more about Snorkel and Snorkel MeTaL at [snorkel.stanford.edu](snorkel.stanford.edu)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
